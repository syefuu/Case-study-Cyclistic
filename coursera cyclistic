{"cells":[{"source":"<a href=\"https://www.kaggle.com/code/seifallahraach/case-study-cyclistic?scriptVersionId=259105906\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","id":"95c4d057","metadata":{"papermill":{"duration":0.002262,"end_time":"2025-08-30T19:02:04.838043","exception":false,"start_time":"2025-08-30T19:02:04.835781","status":"completed"},"tags":[]},"source":["# CASE STUDY : CYCLISTIC BIKE-SHARE \n","\n","This is a documentation for my first case study \"CYCLISTIC BAKE-SHARE\".\n","\n","This link contain the final HTML report made in RStudio : http://rpubs.com/syefu/1339303 \n","\n","And you can also find it on Github :  https://github.com/syefuu/Case-study-Cyclistic\n","\n","## Business understanding \n","\n","**What is Cyclistic?**\n","\n","Cyclistic is a bike share company in Chicago, it's a program that feature more than 5800 bicycles and 600 docking stations.\n","\n","**What do we want to know from this case study?**\n","\n","- Understand how casual riders and annual members use cyclistic bikes differently.\n","- Design a new marketing strategy to convert casual riders into annual members.\n","\n","### Guiding questions\n","1. How do annual members and casual riders use Cyclistic bikes differently?\n","2. Why would casual riders buy Cyclistic annual memberships?\n","3. How can Cyclistic use digital media to influence casual riders to become members?\n","\n","## Business Task\n","\n","**Analyze the differences in usage between annual members and casual riders to identify opportunities to convert casual riders to annual members.**\n","\n","## Data understanding\n","\n","- Data source : Coursera – Cyclistic Case Study\n","- Organization : Tabular format with ride ID, start/end times, start/end station names and IDs, user types, bike type, and the longitude and - latitude of each station\n","- Bias/credibility (ROCCC) : Reliable, original, comprehensive, current, cited but some missing and inconsistent data exist\n","- Licensing, privacy, security and accessibility\tData publicly released and accessible for analysis\n","- Data integrity verification : Checked for unrealistic trip durations, null values, duplicates and standardized the characters type\n","- Limitation : Lack of demographic information\n","- Tools chosen : Google Sheets,SQL server management studio, Tableau, and R\n","\n","## Prepare data for analysis\n","\n","In this section, i will document the entire cleaning process in **Google sheets**, **SQL server management studio** and **RStudio**.\n","\n","### Spreadsheets : Google Sheets\n","\n","Important note : Our dataset is too large to import it all in a single spreadsheet so im going to explain how the cleaning process is executed in google sheet for documentation sake then i will translate my work to SQL so i can work with the data i was provided and continue my analysis.\n","\n","#### Data manipulation \n","\n","##### Add columns ride_length, day_of_week and clean them\n","\n","In order to help us answer the business task, we need to add two new columns to our dataset : \n","\n","I - **ride_length** : Duration of the Trip\n","\n","  1. Format columns: Convert **started_at** and **ended_at** columns to **Date time** format.  \n","  2. Calculate ride_length: Subtract **started_at** from **ended_at** to get the trip duration.  \n","  3. Format duration: Convert the resulting **ride_length** into **duration** format.  \n","  4. Verify results: we will notice some negative values and values that are either too small or too large to be considered a        correct values for a trip duration, let's clean them : \n","        - **Correct negative values:** we will apply **ABS()** to convert negative durations to positive ones.  \n","        - **Remove unrealistic values:** we will delete records where the duration is greater than 1 day, and where the                    duration is lesser than 60 seconds by using the **filtering function** of google sheet.  \n","  5. Final verification: we will perform summary checks and spot-check sample records to confirm cleanliness.\n","  \n","II - **day_of_week** : Day of the Trip\n","\n","  We will use **WEEKDAY()** in a new column to get the day of the week as a number from 1 (Sunday) to 7 (Saturday). \n","  \n","Let's continue the cleaning for the rest of the data we have :\n","\n","##### Station names cleaning\n","\n","When looking at the data, we can notice empty cells in the station names and ids, let's correct them :\n","\n","  1. We will delete empty start station names/ ids, and end station names/ ids using the **filtering function**.  \n","  2. We will standardize station names using **PROPER(TRIM())** to remove extra spaces and capitalize correctly.  \n","  3. we will Check for spelling errors: we will create a **pivot table** with station names as rows and count of station names       as values. \n","\n","##### Remove duplicates \n","\n","Finally, let's remove the rows that contain the same **ride_id** because its supposed to be unique for each ride, it is easily done by going to **Data → Data cleanup → Remove duplicates**.\n","\n","This is the end of the section about google sheet, we will now work on SQL server management studio.\n","\n","### SQL : SQL Server Management Studio\n","\n","#### Data manipulation\n","\n","First, we need to import the datasets to the SQL server management studio (SSMS). We have 12 CSV files.\n","\n","I will start by creating a temporary table, **all_trips_tmp**, where i would import all the data using **CREATE TABLE**:\n","\n","**Why a temporary table?**\n","\n","When i tried to import the second file of the data, i got an error related to the **started_at** column so i made a temporary table that will hold all our data with **NVARCHAR type**.\n","\n","<pre style=\"backgound-color:#f4f4f4; padding:15px;border-radius:8px;border:1px solid #ccc; overflow-x:auto;\">\n","-- Create temporary table all_trips_tmp\n","\n","CREATE TABLE all_trips_tmp (\n","    ride_id NVARCHAR(100),\n","    rideable_type NVARCHAR(100),\n","    started_at NVARCHAR(50),\n","    ended_at NVARCHAR(50),\n","    start_station_name NVARCHAR(255),\n","    start_station_id NVARCHAR(50),\n","    end_station_name NVARCHAR(255),\n","    end_station_id NVARCHAR(50),\n","    start_lat NVARCHAR(50),\n","    start_lng NVARCHAR(50),\n","    end_lat NVARCHAR(50),\n","    end_lng NVARCHAR(50),\n","    member_casual NVARCHAR(50)\n",");\n","</code>\n","</pre>\n","\n","\n","Now i will import the data using **BULK INSERT** ( this query will help import the files one by one so i can make sure it was imported correctly)\n","\n","<pre style=\"backgound-color:#f4f4f4; padding:15px;border-radius:8px;border:1px solid #ccc; overflow-x:auto;\">\n","-- Import data\n","\n","BULK INSERT all_trips_tmp\n","FROM 'File_path\\202101-divvy-tripdata.csv'\n","WITH (\n","    FIRSTROW = 2,\n","    FIELDTERMINATOR = ',',\n","    ROWTERMINATOR = '0x0a',\n","    TABLOCK,\n","    CODEPAGE = '65001'  -- handles UTF-8 if needed\n",");\n","\n","BULK INSERT all_trips_tmp\n","FROM 'File_path\\202102-divvy-tripdata.csv'\n","WITH (\n","    FIRSTROW = 2,\n","    FIELDTERMINATOR = ',',\n","    ROWTERMINATOR = '0x0a',\n","    TABLOCK,\n","    CODEPAGE = '65001'\n","); -- ...We will execute the same query for the 12 files we have.\n","</code>\n","</pre>\n","\n","\n","Now let's create the table, **all_trips** That we will work on for cleaning with the proper data type :\n","\n","<pre style=\"backgound-color:#f4f4f4; padding:15px;border-radius:8px;border:1px solid #ccc; overflow-x:auto;\">\n","-- Create all_trips table with the proper data type\n","\n","CREATE TABLE all_trips (\n","    ride_id NVARCHAR(100),\n","    rideable_type NVARCHAR(100),\n","    started_at DATETIME,\n","    ended_at DATETIME,\n","    start_station_name NVARCHAR(255),\n","    start_station_id NVARCHAR(50),\n","    end_station_name NVARCHAR(255),\n","    end_station_id NVARCHAR(50),\n","    start_lat FLOAT,\n","    start_lng FLOAT,\n","    end_lat FLOAT,\n","    end_lng FLOAT,\n","    member_casual NVARCHAR(50)\n",");\n","</code>\n","</pre>\n","\n","Let's insert the data into **all_trips** by using **INSERT INTO** : \n","\n","<pre style=\"backgound-color:#f4f4f4; padding:15px;border-radius:8px;border:1px solid #ccc; overflow-x:auto;\">\n","-- Insert the data into our table \n","INSERT INTO all_trips\n","SELECT\n","    ride_id,\n","    rideable_type,\n","    TRY_CONVERT(DATETIME, started_at) AS started_at,\n","    TRY_CONVERT(DATETIME, ended_at) AS ended_at,\n","    start_station_name,\n","    start_station_id,\n","    end_station_name,\n","    end_station_id,\n","    TRY_CONVERT(FLOAT, start_lat) AS start_lat,\n","    TRY_CONVERT(FLOAT, start_lng) AS start_lng,\n","    TRY_CONVERT(FLOAT, end_lat) AS end_lat,\n","    TRY_CONVERT(FLOAT, end_lng) AS end_lng,\n","    member_casual\n","FROM all_trips_tmp;\n","</code>\n","</pre>\n","\n","\n","We will check the data by comparing the count of the rows between **all_trips** and **all_trips_tmp**, then we will check a sample from our table :\n","\n","<pre style=\"backgound-color:#f4f4f4; padding:15px;border-radius:8px;border:1px solid #ccc; overflow-x:auto;\">\n","-- Compare how many rows we have between all_trips and all_trips_tmp\n","\n","SELECT COUNT(*) AS total_trips FROM all_trips;\n","SELECT COUNT(*) AS total_temp_rows FROM all_trips_tmp;\n","\n","-- Check a sample of the data\n","\n","SELECT TOP 10 * FROM all_trips;\n","</code>\n","</pre>\n","\n","Now, it's time to start the cleaning process, we can notice that there are a few null values on our table, we will use **DELETE FROM** to get rid of them, we will also standardize the station names so they will be easy to clean by using **UPPER(LTRIM(RTRIM()))** and we will remove the duplicates rides :\n","\n","<pre style=\"backgound-color:#f4f4f4; padding:15px;border-radius:8px;border:1px solid #ccc; overflow-x:auto;\">\n","-- Delete null values from started_at and ended_at\n","\n","DELETE FROM all_trips\n","WHERE started_at IS NULL OR ended_at IS NULL;\n","\n","-- Delete where ride_id is null\n","\n","DELETE FROM all_trips\n","WHERE LTRIM(RTRIM(ride_id)) = '';\n","\n","-- Standardize names : remove extra spaces and capitalize\n","\n","UPDATE all_trips\n","SET start_station_name = UPPER(LTRIM(RTRIM(start_station_name))),\n","    end_station_name   = UPPER(LTRIM(RTRIM(end_station_name)));\n","\n","-- Remove duplicates\n","\n","WITH CTE AS (\n","    SELECT *, ROW_NUMBER() OVER (PARTITION BY ride_id ORDER BY ride_id) AS rn\n","    FROM all_trips\n",")\n","DELETE FROM CTE WHERE rn > 1;\n","\n","--  Delete empty station names\n","\n","DELETE FROM all_trips\n","WHERE start_station_name = '' OR end_station_name = '';\n","</code>\n","</pre>\n","\n","Like we did in the spreadsheet section, we are going to add two new columns that will help us answer the business task, **ride_length** and **day_of_week** by using **ALTER TABLE /ADD** to create our new columns and **UPDATE /SET** to set the values :\n","\n","<pre style=\"backgound-color:#f4f4f4; padding:15px;border-radius:8px;border:1px solid #ccc; overflow-x:auto;\">\n","-- Add a new column for ride_length in seconds\n","\n","ALTER TABLE all_trips\n","ADD ride_length_sec INT;\n","\n","-- Calculate ride_length as the difference between started_at and ended_at\n","\n","UPDATE all_trips\n","SET ride_length_sec = DATEDIFF(SECOND, started_at, ended_at);\n","</code>\n","</pre>\n","\n","Like previously, when checking the new column, we will find some negative values and unrealistic durations, so let's clean our new column :\n","\n","<pre style=\"backgound-color:#f4f4f4; padding:15px;border-radius:8px;border:1px solid #ccc; overflow-x:auto;\">\n","-- Convert negative durations to positive\n","\n","UPDATE all_trips\n","SET ride_length_sec = ABS(ride_length_sec);\n","\n","--  Remove unrealistic durations\n","\n","DELETE FROM all_trips\n","WHERE ride_length_sec <= 60            -- delete rides shorter than 1 minute\n","   OR ride_length_sec > 86400;         -- delete rides longer than 1 day\n","</code>\n","</pre>\n","\n","Let's add our second new column **day_of_week** and also **day_name** :\n","\n","<pre style=\"backgound-color:#f4f4f4; padding:15px;border-radius:8px;border:1px solid #ccc; overflow-x:auto;\">\n","-- Add a new column for day_of_week\n","\n","ALTER TABLE all_trips\n","ADD day_of_week INT;\n","\n","-- Extract the day from the column started_at\n","\n","UPDATE all_trips\n","SET day_of_week = DATEPART(WEEKDAY, started_at);\n","\n","-- Add a new column for day_name\n","\n","ALTER TABLE all_trips\n","ADD day_name NVARCHAR(10);\n","\n","-- Extract the day name from the column started_at\n","\n","UPDATE all_trips\n","SET day_name = DATENAME(WEEKDAY, started_at);\n","</code>\n","</pre>\n","\n","Let's do more verification of the data :\n","\n","<pre style=\"backgound-color:#f4f4f4; padding:15px;border-radius:8px;border:1px solid #ccc; overflow-x:auto;\">\n","-- Check for start station names inconsistencies\n","\n","SELECT start_station_name, COUNT(*) AS trips\n","FROM all_trips\n","GROUP BY start_station_name\n","ORDER BY start_station_name ;\n","\n","-- There is a station name 351 that contain one trip ride that isn't consistent with the rest of the data so i will delete it\n","-- Delete station name 351\n","\n","DELETE FROM all_trips\n","WHERE start_station_name = '351';\n","\n","-- Check for end station names inconsistencies\n","\n","SELECT end_station_name, COUNT(*) AS trips\n","FROM all_trips\n","GROUP BY end_station_name\n","ORDER BY end_station_name ;\n","\n","-- Final check of the data for null values  \n","\n","SELECT\n","  COUNT(*) AS total_rows,\n","  COUNT(CASE WHEN ride_id IS NULL OR LTRIM(RTRIM(ride_id)) = '' THEN 1 END) AS empty_ride_id,\n","  COUNT(CASE WHEN start_station_name IS NULL OR LTRIM(RTRIM(start_station_name)) = '' THEN 1 END) AS empty_start_station_name,\n","  COUNT(CASE WHEN end_station_name IS NULL OR LTRIM(RTRIM(end_station_name)) = '' THEN 1 END) AS empty_end_station_name,\n","  COUNT(CASE WHEN rideable_type IS NULL OR LTRIM(RTRIM(rideable_type)) = '' THEN 1 END) AS empty_rideable_type,\n","  COUNT(CASE WHEN member_casual IS NULL OR LTRIM(RTRIM(member_casual)) = '' THEN 1 END) AS empty_member_casual\n","FROM all_trips;\n","\n","-- See a preview of the data\n","\n","SELECT TOP 10 * FROM all_trips;\n","</code>\n","</pre>\n","\n","After a final review, the dataset is now clean and ready for analysis.\n","\n","#### Next Steps : Tableau\n","\n","I used the cleaned data from the table **all_trips** in **Tableau** for visualization ( you can import the data from sql directly to tableau without the need to export it).\n","\n","These are the dashboards i made :\n","\n","_ Bike type overview : members vs casuals : See the riders activity based on the type of bikes (https://public.tableau.com/app/profile/seifallah.raach/viz/Biketypeoverviewmembersvscasuals/biketypeoverview) <br>\n","\n","_ Seasonal approach : See the riders activity by seasons.(https://public.tableau.com/app/profile/seifallah.raach/viz/Seasonalapproach/seasonalapproach?publish=yes) <br>\n","\n","_ Weekends vs weekdays : See the difference in activity between the weekends and weekdays by the rider type.  (https://public.tableau.com/app/profile/seifallah.raach/viz/WeekendsvsWeekdays/Weekdaysvsweekends?publish=yes)  <br>\n","\n","_ Riders activity : See the stations where we have the most traffic between casual riders and members.\n","(https://public.tableau.com/app/profile/seifallah.raach/viz/Ridersactivity/stationsapproach?publish=yes) <br>\n","\n","## Data analysis \n","\n","Let's summarize all our observations : \n","\n","1. Classic bikes are popular for both rider types, and casual riders use docked bikes unlike members.\n","2. For both members and casuals, the most active time is between 16h and 18h.\n","3. Docked bikes are only used by casuals for long trips.\n","4. Casual riders take longer trips than members, whereas members take more trips than casuals.\n","5. Casual riders are more active on weekends, unlike members whose number of trips remains mostly consistent.\n","6. Casual riders and members take longer trips on weekends.\n","7. Top 3 routes more frequented by casual riders:\n","            Streeter Dr & Grand Ave → Streeter Dr & Grand Ave\n","            Millennium Park → Millennium Park\n","            Michigan Ave & Oak St → Michigan Ave & Oak St\n","8. Riders are more active in spring, summer, and fall, especially summer; casual riders are more active in summer than members.\n","9. Casual riders take longer trips in spring, while members are more consistent in trip duration with a small increase in spring.\n","\n","## Top recommandations\n","\n","We reached the end of our case study, based on the observations i made these are my top recommendations :\n","\n","1. Promote cost saving for frequent casual riders who rides on weekends and during spring and have longer trips.\n","2. Targeted marketing campaigns during summer and spring for casual riders with ads (mail, phone..).\n","3. Targeted marketing campaign on the most popular routes frequented by the casual riders on rush hours (flyers, posters..).\n","4. Offer promotions during the peak seasons and weekends.\n","5. Promote the docked bikes and what they offer as benefits by being a member for using them.\n","6. Showcase the convenience of a membership to casual riders for longer trips.\n","7. Showcase member benefits when taking longer trips.\n","\n","## Presentation\n","\n","I will wrap up this study by sharing the link to my presentation, i hope my work was informative :\n","\n","Final presentation :\n","\n","https://www.canva.com/design/DAGw_GESHy8/oPsyoyKQD6dELPLBSaK-YA/edit?utm_content=DAGw_GESHy8&utm_campaign=designshare&utm_medium=link2&utm_source=sharebutton)\n","\n","Thank you for reading <br>\n","\n","Seifallah Raach <br>\n"]}],"metadata":{"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":31089,"isGpuEnabled":false,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.13"},"papermill":{"default_parameters":{},"duration":6.219604,"end_time":"2025-08-30T19:02:05.259984","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2025-08-30T19:01:59.04038","version":"2.6.0"}},"nbformat":4,"nbformat_minor":5}